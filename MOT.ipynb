{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 设置当前工作目录\n",
    "os.chdir(r'E:\\ML\\Pedestrian_trackingMOT') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "def construct_dataset(dataset_name, max_images_per_id=200):\n",
    "\n",
    "    file_path='MOT15/train/'+dataset_name\n",
    "    gt_file_path = os.path.join(file_path, 'gt/gt.txt')\n",
    "    \n",
    "    df = pd.read_csv(gt_file_path, header=None)\n",
    "\n",
    "    # 帧号、ID、边界框的左上角坐标(x, y)、边界框的宽度和高度、置信度、3D位置信息(x, y, z)\n",
    "    df.columns = [\"frame\", \"id\", \"bb_left\", \"bb_top\", \"bb_width\", \"bb_height\", \"confidence\", \"x3d\", \"y3d\", \"z3d\"]\n",
    "\n",
    "    df_filtered = df[df[\"confidence\"] != 0]\n",
    "\n",
    "    df = df_filtered[[\"frame\", \"id\", \"bb_left\", \"bb_top\", \"bb_width\", \"bb_height\"]]\n",
    "\n",
    "\n",
    "    video_frames_dir=os.path.join(file_path, 'img1')\n",
    "\n",
    "    output_dir = os.path.join('data', 'train')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Iterate over unique IDs\n",
    "    for person_id in df['id'].unique():\n",
    "        person_df = df[df['id'] == person_id]\n",
    "        images_saved = 0\n",
    "\n",
    "        for _, row in person_df.iterrows():\n",
    "            frame_number = int(row['frame'])  # Ensure frame number is an integer\n",
    "            bb_left, bb_top, bb_width, bb_height = int(row['bb_left']), int(row['bb_top']), int(row['bb_width']), int(row['bb_height'])\n",
    "\n",
    "            # Construct image path\n",
    "            frame_path = os.path.join(video_frames_dir, f'{frame_number:06d}.jpg')\n",
    "            \n",
    "            if os.path.exists(frame_path):\n",
    "                # Read and crop image\n",
    "                image = cv2.imread(frame_path)\n",
    "\n",
    "                if bb_left + bb_width > image.shape[1] or bb_top + bb_height > image.shape[0]:\n",
    "                    print(f\"Cropping parameters are out of bounds for the image size. Skipping crop.\")\n",
    "                    continue\n",
    "                \n",
    "                crop = image[bb_top:bb_top+bb_height, bb_left:bb_left+bb_width]\n",
    "                \n",
    "                if crop.size == 0:\n",
    "                    print(\"Cropped image is empty. Skipping resize.\")\n",
    "                    continue\n",
    "                crop = cv2.resize(crop, (240, 480))  # Resize to a consistent size\n",
    "\n",
    "                # Save cropped image\n",
    "                output_path = os.path.join(output_dir, f'{dataset_name}_{person_id}_frame_{frame_number}.jpg')\n",
    "                cv2.imwrite(output_path, crop)\n",
    "                images_saved += 1\n",
    "\n",
    "                # Check if max images per ID is reached\n",
    "                if images_saved >= max_images_per_id:\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the folder: 15085\n"
     ]
    }
   ],
   "source": [
    "# construct_dataset(\"ADL-Rundle-6\")\n",
    "# construct_dataset(\"ADL-Rundle-8\")\n",
    "# construct_dataset(\"ETH-Bahnhof\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 自定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # 生成随机遮挡\n",
    "        masked_image = self.random_mask(image)\n",
    "\n",
    "        if self.transform:\n",
    "            masked_image = self.transform(masked_image)\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, masked_image\n",
    "\n",
    "    def random_mask(self, image):\n",
    "        \"\"\"\n",
    "        随机遮挡图像的 25% 到 40% 区域\n",
    "        \"\"\"\n",
    "        # 将PIL图像转换为numpy数组\n",
    "        np_image = np.array(image)\n",
    "\n",
    "        # 获取图像尺寸\n",
    "        height, width, _ = np_image.shape\n",
    "\n",
    "        mask_percentage = random.uniform(0.25, 0.40)\n",
    "        mask_area = height * width * mask_percentage\n",
    "\n",
    "        # 计算遮挡区域的尺寸 \n",
    "        mask_height = int(np.sqrt(mask_area * height / width))\n",
    "        mask_width = int(mask_height * width / height)\n",
    "\n",
    "\n",
    "        # 随机选择遮挡的起始点\n",
    "        top = random.randint(0, height - mask_height)\n",
    "        left = random.randint(0, width - mask_width)\n",
    "\n",
    "        # 创建遮挡\n",
    "        np_image[top:top + mask_height, left:left + mask_width, :] = 0  # 将遮挡区域设置为黑色\n",
    "\n",
    "        # 将numpy数组转换回PIL图像\n",
    "        masked_image = Image.fromarray(np_image)\n",
    "\n",
    "        return masked_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "#定义模型\n",
    "\n",
    "pretrained_net_vgg=torchvision.models.vgg19(pretrained=True, progress=True)\n",
    "\n",
    "class MyVGG(nn.Module):\n",
    "    def __init__(self,feature_layers):\n",
    "        '''\n",
    "        feature_layers:选取的特征层的索引\n",
    "        '''\n",
    "        super(MyVGG,self).__init__()\n",
    "        self.feature_layers=feature_layers\n",
    "        self.net=pretrained_net_vgg.features[:max(feature_layers)+1]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        用于提取图像的不同尺度特征\n",
    "        '''\n",
    "        features=[]\n",
    "        for i in range(len(self.net)):\n",
    "            x=self.net[i](x)\n",
    "            if i in self.feature_layers:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "# 加载预训练的 ResNet-18\n",
    "pretrained_net_resnet =torchvision.models.resnet18(pretrained=True, progress=True)\n",
    "\n",
    "class MyResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyResNet, self).__init__()\n",
    "\n",
    "        # 使用 ResNet-18 的预训练层\n",
    "        self.layer1 = nn.Sequential(*list(pretrained_net_resnet.children())[:5])  # 到 layer1\n",
    "        self.layer2 = pretrained_net_resnet.layer2\n",
    "        self.layer3 = pretrained_net_resnet.layer3\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "\n",
    "        # 通过 layer1\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        # 通过 layer2 的第一个残差块的两个卷积层\n",
    "        x = self.layer2[0](x)\n",
    "        features.append(x)  # 第一个残差块的输出\n",
    "\n",
    "        x=self.layer2[1](x)\n",
    "        features.append(x)\n",
    "\n",
    "        # 通过 layer3 的第一个残差块\n",
    "        x = self.layer3[0](x)\n",
    "        features.append(x)\n",
    "\n",
    "        # 通过 layer3 的第二个残差块的两个卷积层\n",
    "        x = self.layer3[1](x)\n",
    "        features.append(x)  # 第二个残差块的输出\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 损失函数\n",
    "def feature_loss(features_original, features_masked):\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # 假设features_original和features_masked是特征列表，且长度相同\n",
    "    for feature_original, feature_masked in zip(features_original, features_masked):\n",
    "        total_loss += criterion(feature_original, feature_masked)\n",
    "\n",
    "    # 计算平均损失\n",
    "    average_loss = total_loss / len(features_original)\n",
    "    return average_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor = MyResNet().to(device)  # 左侧固定的网络\n",
    "def train_model(model,dataloader, epochs=10):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, masked_images) in enumerate(dataloader):\n",
    "            # 将图像和遮挡图像移至相同的设备\n",
    "            images = images.to(device)\n",
    "            masked_images = masked_images.to(device)\n",
    "\n",
    "            # 原始模型特征\n",
    "            with torch.no_grad():  # 确保不更新原始模型\n",
    "                original_features = [feature.detach() for feature in feature_extractor(images)]\n",
    "\n",
    "            # 遮挡模型特征\n",
    "            masked_features = model(masked_images)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = feature_loss(original_features, masked_features)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Batch {i+1}/{total_batches}, Batch Loss: {loss.item():.4f}')\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=rgb_mean, std=rgb_std)\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(data_dir='data/train', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model\n",
    "\n",
    "model = MyResNet().to(device)   # 右侧的网络，将被训练\n",
    "\n",
    "model.load_state_dict(torch.load('fine-tune-MyResNet.pth'))\n",
    "\n",
    "# Train\n",
    "train_model(model, dataloader, epochs=5)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'fine-tune-MyResNet_10.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('full-fine-tune-MyResNet.pth',map_location=torch.device('cpu'))\n",
    "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "def preprocess(PIL_img, image_shape):\n",
    "    '''\n",
    "    先更改输入图像的尺寸,然后再将PIL图片转成卷积神经网络接受的输入格式,再在RGB三个通道分别做标准化\n",
    "    因为Resize类要求输入是PIL图片格式,所以我的输入图像默认是PIL图片格式\n",
    "    '''\n",
    "    process = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "\n",
    "    return process(PIL_img).unsqueeze(dim = 0) # (batch_size, 3, H, W)\n",
    "img=Image.open('data/train/ADL-Rundle-6_1_frame_1.jpg')\n",
    "img=preprocess(img,(224,224))\n",
    "img=img.to(device)\n",
    "model.eval()\n",
    "f=model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "data_source = r'MOT15/train/KITTI-13/img1'\n",
    "model = YOLO(r'ultralytics\\yolov8n.pt')\n",
    "results = model(data_source,stream=True,classes=[0],conf=0.65)\n",
    "file_paths=[]\n",
    "for r in results:\n",
    "    if np.array(r.boxes.xyxy).shape[0]==0:\n",
    "        file_paths.append(r.path)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_images(folder_path, extension='jpg'):\n",
    "    files = [file for file in os.listdir(folder_path) if file.lower().endswith(extension.lower())]\n",
    "    files.sort()  # 可选，根据需要排序文件\n",
    "\n",
    "    for i, file in enumerate(files, start=1):\n",
    "        new_file_name = f\"{i:06d}.{extension}\"\n",
    "        old_file_path = os.path.join(folder_path, file)\n",
    "        new_file_path = os.path.join(folder_path, new_file_name)\n",
    "\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        # print(f\"Renamed {file} to {new_file_name}\")\n",
    "\n",
    "folder_path = 'MOT15/train/KITTI-13/img1'  # 替换为图像文件夹的路径\n",
    "rename_images(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import colorsys\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def read_imgs(img_dir):\n",
    "    '''\n",
    "    img_dir:图片文件夹路径\n",
    "    读取图片文件夹中的图片,返回图片的路径列表\n",
    "    '''\n",
    "    imgs=[]\n",
    "    for file_name in sorted(os.listdir(img_dir)):\n",
    "        if file_name.endswith('.jpg') or file_name.endswith('.png'):\n",
    "            image_path = os.path.join(img_dir, file_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            imgs.append(image)\n",
    "    return imgs\n",
    "\n",
    "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "def preprocess(PIL_img, image_shape):\n",
    "    '''\n",
    "    先更改输入图像的尺寸,然后再将PIL图片转成卷积神经网络接受的输入格式,再在RGB三个通道分别做标准化\n",
    "    因为Resize类要求输入是PIL图片格式,所以我的输入图像默认是PIL图片格式\n",
    "    '''\n",
    "    process = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "\n",
    "    return process(PIL_img).unsqueeze(dim = 0) # (batch_size, 3, H, W)\n",
    "\n",
    "class Tracker(object):\n",
    "    def __init__(self,source):\n",
    "        self.boxes_xyxy = [] #box_xxyy(ndarray)\n",
    "        self.crop_imgs=[]   #crop_imgs(list)\n",
    "        self.GT_sequence=[]#{id:box_xxyy}\n",
    "        self.frames_features=[]#{id:features}\n",
    "        self.orig_imgs=read_imgs(source)\n",
    "        self.vgg=MyVGG(feature_layers=[0, 5, 10, 19, 25,28])\n",
    "        self.fine_tune_vgg = MyVGG(feature_layers=[0, 5, 10, 19, 25, 28])\n",
    "        self.fine_tune_vgg.load_state_dict(torch.load('fine-tune-MyVGG.pth'))\n",
    "        self.resnet=MyResNet()\n",
    "        self.fine_tune_resnet= torch.load('full-fine-tune-MyResNet.pth',map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def get_xyxy(self):\n",
    "        return self.boxes_xyxy\n",
    "    \n",
    "    def put_bbox(self,boxes_xyxy):\n",
    "        '''\n",
    "        boxes_xyxy:当前帧的所有行人边界框\n",
    "        '''\n",
    "        self.boxes_xyxy.append(np.array(boxes_xyxy))\n",
    "\n",
    "    def crop_img(self,idx):\n",
    "        '''\n",
    "        idx:要裁剪图像的索引与这张图象对应的边界框的索引\n",
    "        裁剪图片,获得行人的图片,存储在crop_imgs中\n",
    "        '''\n",
    "        boxes_xyxy=self.boxes_xyxy[idx]\n",
    "        img=self.orig_imgs[idx]\n",
    "        cropped_image_list=[]\n",
    "        for _, bbox in enumerate(boxes_xyxy):\n",
    "            x1, y1, x2, y2 = map(int, [bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "            cropped_image = img[y1:y2, x1:x2]\n",
    "            cropped_image_list.append(cropped_image)\n",
    "\n",
    "        self.crop_imgs.append(cropped_image_list)\n",
    "        return cropped_image_list\n",
    "\n",
    "    @property\n",
    "    def get_crop_imgs(self):\n",
    "        return self.crop_imgs\n",
    "\n",
    "\n",
    "    \n",
    "    def compute_jaccard(self,box1,box2):\n",
    "        \"\"\"计算两个边界框的IoU\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "\n",
    "\n",
    "        intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "        return intersection_area / union_area\n",
    "\n",
    "\n",
    "    def trackByIoU(self,iou=0.5,interval=1):\n",
    "        '''\n",
    "        iou:IoU阈值\n",
    "        interval:间隔帧数\n",
    "        计算当前帧某一行人与前interval帧所有行人的IoU的极大值,且该值大于阈值则认为是同一个行人\n",
    "        如果没有符合条件的，就认为是新的行人,分配新的ID,否则分配与之IoU最大的行人的ID\n",
    "        '''\n",
    "        boxes_xyxy=self.boxes_xyxy[-1]\n",
    "        last_interval_frame_results=[]\n",
    "        if self.GT_sequence == []:\n",
    "            self.GT_sequence.append(dict(enumerate(boxes_xyxy)))\n",
    "        else:\n",
    "            last_interval_frame_results=self.GT_sequence[-interval:]\n",
    "\n",
    "            last_ids = []\n",
    "            for result in reversed(last_interval_frame_results):\n",
    "                for last_id in result.keys():\n",
    "                    if last_id not in last_ids:\n",
    "                        last_ids.append(last_id)\n",
    "            max_id=max(last_ids)\n",
    "\n",
    "            # 匈牙利算法\n",
    "            cost_matrix = []\n",
    "            for current_id, current_bbox in enumerate(boxes_xyxy):\n",
    "                row = []\n",
    "                appeared_ids=[]\n",
    "                for result in reversed(last_interval_frame_results):\n",
    "                    for last_id, last_bbox in result.items():\n",
    "                        if last_id not in appeared_ids:\n",
    "                            appeared_ids.append(last_id)\n",
    "                            IoU = self.compute_jaccard(current_bbox, last_bbox)                            \n",
    "                            cost=1-IoU\n",
    "                            row.append(cost)\n",
    "                cost_matrix.append(row)\n",
    "\n",
    "\n",
    "                \n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            \n",
    "            best_match = {}  # 最终匹配结果\n",
    "            for current_id, col in zip(row_ind, col_ind):\n",
    "                if cost_matrix[current_id][col] < 1 - iou:\n",
    "                    matched_id = last_ids[col]\n",
    "                    best_match[matched_id] = current_id\n",
    "\n",
    "            for current_id, _ in enumerate(boxes_xyxy):\n",
    "                if current_id not in best_match.values():\n",
    "                    max_id += 1\n",
    "                    best_match[max_id] = current_id\n",
    "            \n",
    "            best_match = {last_id: boxes_xyxy[current_id] for last_id, current_id in best_match.items()}\n",
    "\n",
    "            self.GT_sequence.append(best_match)\n",
    "        \n",
    "\n",
    "    def compute_cosine_similarity(self,features1,features2):\n",
    "        '''\n",
    "        features1:行人1的特征,列表\n",
    "        features2:行人2的特征,列表\n",
    "        返回两个行人特征的余弦相似度\n",
    "        '''\n",
    "        cosine_similarities = []\n",
    "        for feature1_map, feature2_map in zip(features1, features2):\n",
    "            # 展平特征图\n",
    "            vec1 = feature1_map.view(-1)\n",
    "            vec2 = feature2_map.view(-1)\n",
    "\n",
    "            # 计算余弦相似度\n",
    "            cosine_sim = F.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))\n",
    "            cosine_similarities.append(cosine_sim)\n",
    "\n",
    "        avg_cosine_similarity = torch.mean(torch.stack(cosine_similarities))\n",
    "        return avg_cosine_similarity.item()  \n",
    "\n",
    "\n",
    "    def trackBYnet(self,fine_tune=False,threshold=0.25,interval=1,net_type='vgg'):\n",
    "        '''\n",
    "        fine_tune:是否对VGG19进行微调\n",
    "        threshold:余弦相似度阈值\n",
    "        interval:间隔帧数\n",
    "        '''\n",
    "        idx=len(self.boxes_xyxy)-1\n",
    "        cropped_images_list=self.crop_img(idx)\n",
    "        current_frame_features={}#id:features\n",
    "\n",
    "        if fine_tune:\n",
    "            if net_type=='vgg':\n",
    "                Net=self.fine_tune_vgg\n",
    "            else:\n",
    "                Net=self.fine_tune_resnet\n",
    "        else:\n",
    "            if net_type=='vgg':\n",
    "                Net=self.vgg\n",
    "            else:\n",
    "                Net=self.resnet\n",
    "            \n",
    "        Net.eval()\n",
    "        #获得当前帧的所有行人特征\n",
    "        for i, cropped_image in enumerate(cropped_images_list):\n",
    "            #转为PIL格式的image进行预处理（BGR->RGB）\n",
    "            cropped_image = Image.fromarray(cropped_image[..., ::-1])\n",
    "            cropped_image=preprocess(cropped_image,(224, 224))\n",
    "            features=Net(cropped_image)\n",
    "            current_frame_features[i]=features\n",
    "\n",
    "\n",
    "        if self.frames_features == []:\n",
    "            #初始化的id仅对初始帧有效，后续id要根据匹配结果进行更新\n",
    "            self.frames_features.append(current_frame_features)\n",
    "        else:\n",
    "            if len(self.frames_features)>interval:\n",
    "                self.frames_features.pop(0) #删除最早的一帧ps:节约一下内存，不然会爆\n",
    "\n",
    "            last_interval_frame_features=self.frames_features[-interval:]\n",
    "\n",
    "            \n",
    "            last_ids = []  # 用于存储成本矩阵中每一列对应的行人ID\n",
    "            for frame_feature in reversed(last_interval_frame_features):\n",
    "                for last_id in frame_feature.keys():\n",
    "                    if last_id not in last_ids:\n",
    "                        last_ids.append(last_id)\n",
    "\n",
    "            max_id=max(last_ids)\n",
    "\n",
    "            cost_matrix = []\n",
    "            \n",
    "            for current_ID, current_features in current_frame_features.items():\n",
    "                row = []\n",
    "                appeared_ids=[]\n",
    "                for frame_feature in reversed(last_interval_frame_features):\n",
    "                    for last_id, last_features in frame_feature.items():\n",
    "                        '''\n",
    "                        cosine_similarity = self.compute_cosine_similarity(current_features, last_features)\n",
    "                        cost = 1 - cosine_similarity  # 转换为成本\n",
    "                        row.append(cost)  \n",
    "                        feature_loss=feature_loss(current_features,last_features)\n",
    "                        '''\n",
    "                        if last_id not in appeared_ids:\n",
    "                            appeared_ids.append(last_id)\n",
    "                            cosine_similarity = self.compute_cosine_similarity(current_features, last_features)\n",
    "                            cost = 1 - cosine_similarity\n",
    "                            row.append(cost) \n",
    "                            \n",
    "                cost_matrix.append(row)\n",
    "            \n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "            \n",
    "            best_match = {}  # 最终匹配结果\n",
    "            for current_id, col in zip(row_ind, col_ind):\n",
    "                if cost_matrix[current_id][col] < 1 - threshold:\n",
    "                    matched_id = last_ids[col]\n",
    "                    best_match[matched_id] = current_id\n",
    "\n",
    "\n",
    "            for current_id in current_frame_features.keys():\n",
    "                if current_id not in best_match.values():\n",
    "                    max_id += 1\n",
    "                    best_match[max_id] = current_id\n",
    "\n",
    "            best_match = {last_id: current_frame_features[current_id] for last_id, current_id in best_match.items()}  \n",
    "            self.frames_features.append(best_match) \n",
    "\n",
    "        #存放,ID：位置结果\n",
    "        current_bbox=self.boxes_xyxy[idx]\n",
    "        current_bbox=dict(enumerate(current_bbox))\n",
    "        self.GT_sequence.append(dict(zip(self.frames_features[-1].keys(),current_bbox.values())))\n",
    "        print(self.GT_sequence[-1])\n",
    "\n",
    "\n",
    "\n",
    "    def trackByIoU_net(self,iou=0.5,interval=1,threshold=0.8,fine_tune=True,net_type='vgg'):\n",
    "        '''\n",
    "        iou:IoU阈值\n",
    "        interval:间隔帧数\n",
    "        '''\n",
    "        boxes_xyxy=self.boxes_xyxy[-1]\n",
    "        last_interval_frame_results=[]\n",
    "        if self.GT_sequence == []:\n",
    "            self.GT_sequence.append(dict(enumerate(boxes_xyxy)))\n",
    "        else:\n",
    "            last_interval_frame_results=self.GT_sequence[-interval:]\n",
    "\n",
    "            last_ids = []\n",
    "            for result in reversed(last_interval_frame_results):\n",
    "                for last_id in result.keys():\n",
    "                    if last_id not in last_ids:\n",
    "                        last_ids.append(last_id)\n",
    "            max_id=max(last_ids)\n",
    "\n",
    "            # 匈牙利算法\n",
    "            #但是这么做的话，会有可能错误匹配：比如上一帧检测出两个人，这一帧有一个人没被检测到，但是又有前面被遮挡的人出现，导致画面中总人数没变\n",
    "            # 但是那个没被检测出来的行人的ID会被分配给这个新出现的人，这样就会导致错误匹配\n",
    "            cost_matrix = []\n",
    "            for current_id, current_bbox in enumerate(boxes_xyxy):\n",
    "                row = []\n",
    "                appeared_ids=[]\n",
    "                for result in reversed(last_interval_frame_results):\n",
    "                    for last_id, last_bbox in result.items():\n",
    "                        if last_id not in appeared_ids:\n",
    "                            appeared_ids.append(last_id)\n",
    "                            IoU = self.compute_jaccard(current_bbox, last_bbox)\n",
    "                            cost=1-IoU\n",
    "                            row.append(cost)\n",
    "                cost_matrix.append(row)\n",
    "                        \n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            # 候选匹配结果\n",
    "            potential_matches={}\n",
    "\n",
    "            best_match = {}  # 最终匹配结果\n",
    "            for current_id, col in zip(row_ind, col_ind):\n",
    "                matched_id = last_ids[col]\n",
    "                if cost_matrix[current_id][col] < 1 - iou:\n",
    "                    best_match[matched_id] = current_id\n",
    "                else:\n",
    "                    potential_matches[matched_id]=current_id\n",
    "            \n",
    "\n",
    "            # if len(self.boxes_xyxy) ==238:\n",
    "            #     print(best_match,potential_matches)\n",
    "                \n",
    "            \n",
    "            if fine_tune:\n",
    "                if net_type=='vgg':\n",
    "                    Net=self.fine_tune_vgg\n",
    "                else:\n",
    "                    Net=self.fine_tune_resnet\n",
    "            else:\n",
    "                if net_type=='vgg':\n",
    "                    Net=self.vgg\n",
    "                else:\n",
    "                    Net=self.resnet\n",
    "            \n",
    "\n",
    "            \n",
    "            # 我们认为IoU匹配结果是可靠的，也就是在一段时间内没被遮挡的人的匹配是鲁棒的，\n",
    "            # 出现遮挡会导致有追踪目标短暂消失，因此在其再次出现时，IoU的值会低于阈值/或其不是最优解,导致被识别为新目标\n",
    "            # 因此，对于那些因为IoU匹配度过低被筛选掉的人，我们需要再次判断是否在之前出现过，通过检查其特征与last_ids（interval的长度决定了目标最多可以消失多久）\n",
    "            # 中未被匹配到的人的特征的余弦相似度来决定匹配\n",
    "            # 为增加匹配鲁棒性，增加纠错机制，防止因为IoU跟踪错之后后面一直跟踪错。\n",
    "            \n",
    "            not_matched_last_ids=[]\n",
    "            for last_id in last_ids:\n",
    "                if last_id not in best_match.keys():\n",
    "                    not_matched_last_ids.append(last_id)\n",
    "\n",
    "            current_ids=[]\n",
    "            for current_id in potential_matches.values():\n",
    "                current_ids.append(current_id)\n",
    "            \n",
    "            vgg_cost_matrix = []\n",
    "\n",
    "            if potential_matches != {}:\n",
    "                for _, current_id in potential_matches.items():\n",
    "                    idx=len(self.boxes_xyxy)-1\n",
    "                    current_image=self.orig_imgs[idx]\n",
    "                    current_bbox=boxes_xyxy[current_id]\n",
    "                    row = []\n",
    "                    for id in not_matched_last_ids:\n",
    "                        i=0\n",
    "                        idx=len(self.boxes_xyxy)-1\n",
    "                        for result in reversed(last_interval_frame_results):\n",
    "                            i+=1\n",
    "                            if id in result.keys():\n",
    "                                last_bbox=result[id]\n",
    "                                break\n",
    "                            \n",
    "                        idx=idx-i\n",
    "\n",
    "                        last_image=self.orig_imgs[idx]\n",
    "                        last_cropped_image=self.crop_one_img(last_image,last_bbox)\n",
    "                        current_cropped_image=self.crop_one_img(current_image,current_bbox)\n",
    "                        cosine_similarity=self.caculate_TwoImages_similarity(Net,last_cropped_image,current_cropped_image)\n",
    "                        row.append(1-cosine_similarity)\n",
    "                    vgg_cost_matrix.append(row)\n",
    "\n",
    "                row_ind, col_ind = linear_sum_assignment(vgg_cost_matrix)\n",
    "\n",
    "                if len(self.boxes_xyxy) == 84 or len(self.boxes_xyxy) == 122 :\n",
    "                    print(vgg_cost_matrix,not_matched_last_ids)\n",
    "                    \n",
    "                # 阈值过低，会导致新出现的人被匹配到之前已经消失的人的编号上\n",
    "                # 阈值过高，会导致之前被遮挡的人再次出现时无法被匹配到。\n",
    "                for row, col in zip(row_ind, col_ind):\n",
    "                    current_id=current_ids[row]\n",
    "                    matched_id=not_matched_last_ids[col]\n",
    "                    if vgg_cost_matrix[row][col]<1-threshold:\n",
    "                        best_match[matched_id]=current_id\n",
    "\n",
    "\n",
    "            for current_id, _ in enumerate(boxes_xyxy):\n",
    "                if current_id not in best_match.values():\n",
    "                    max_id += 1\n",
    "                    best_match[max_id] = current_id\n",
    "            \n",
    "            best_match = {last_id: boxes_xyxy[current_id] for last_id, current_id in best_match.items()}\n",
    "\n",
    "            self.GT_sequence.append(best_match)\n",
    "\n",
    "\n",
    "\n",
    "    def crop_one_img(self,orig_img,box_xyxy):\n",
    "        '''\n",
    "        orig_img:原始图像\n",
    "        box_xyxy:行人边界框\n",
    "        '''\n",
    "        x1, y1, x2, y2 = map(int, [box_xyxy[0], box_xyxy[1], box_xyxy[2], box_xyxy[3]])\n",
    "        cropped_image = orig_img[y1:y2, x1:x2]\n",
    "        return cropped_image    \n",
    "\n",
    "    def caculate_TwoImages_similarity(self,feature_extractor,img1,img2):\n",
    "        '''\n",
    "        img1:图像1\n",
    "        img2:图像2\n",
    "        计算两张图像的余弦相似度\n",
    "        '''\n",
    "        img1 = Image.fromarray(img1[..., ::-1])\n",
    "        img2 = Image.fromarray(img2[..., ::-1])\n",
    "        img1=preprocess(img1,(224,224))\n",
    "        img2=preprocess(img2,(224,224))\n",
    "        device = next(feature_extractor.parameters()).device\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        img1_features=feature_extractor(img1)\n",
    "        img2_features=feature_extractor(img2)\n",
    "        return self.compute_cosine_similarity(img1_features,img2_features)\n",
    "    \n",
    "\n",
    "    def id_to_color(self,id):\n",
    "        # 将ID映射到0到1之间的色相值\n",
    "        hue = id * 0.618033988749895 % 1\n",
    "        # 转换为RGB颜色\n",
    "        r, g, b = colorsys.hsv_to_rgb(hue, 1, 1)\n",
    "        return int(r * 255), int(g * 255), int(b * 255)\n",
    "    \n",
    "\n",
    "    def save_result(self, folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        for index, (img, current_bbox) in enumerate(zip(self.orig_imgs, self.GT_sequence)):\n",
    "            for id, bbox in current_bbox.items():\n",
    "                # 为每个ID生成一个颜色\n",
    "                color = self.id_to_color(id)\n",
    "\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color, 5)\n",
    "                cv2.putText(img, str(id), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5)\n",
    "\n",
    "            filename = os.path.join(folder_path, f'{index + 1:06d}.jpg')\n",
    "            cv2.imwrite(filename, img)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOTBenchmark(object):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000001.jpg: 384x640 4 persons, 135.9ms\n",
      "image 2/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000002.jpg: 384x640 4 persons, 114.9ms\n",
      "image 3/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000003.jpg: 384x640 4 persons, 115.8ms\n",
      "image 4/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000004.jpg: 384x640 5 persons, 104.4ms\n",
      "image 5/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000005.jpg: 384x640 4 persons, 102.9ms\n",
      "image 6/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000006.jpg: 384x640 4 persons, 106.7ms\n",
      "image 7/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000007.jpg: 384x640 3 persons, 102.6ms\n",
      "image 8/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000008.jpg: 384x640 4 persons, 102.0ms\n",
      "image 9/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000009.jpg: 384x640 5 persons, 107.1ms\n",
      "image 10/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000010.jpg: 384x640 4 persons, 109.7ms\n",
      "image 11/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000011.jpg: 384x640 4 persons, 104.0ms\n",
      "image 12/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000012.jpg: 384x640 5 persons, 100.1ms\n",
      "image 13/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000013.jpg: 384x640 5 persons, 101.2ms\n",
      "image 14/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000014.jpg: 384x640 4 persons, 115.1ms\n",
      "image 15/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000015.jpg: 384x640 4 persons, 103.1ms\n",
      "image 16/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000016.jpg: 384x640 5 persons, 101.7ms\n",
      "image 17/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000017.jpg: 384x640 5 persons, 103.1ms\n",
      "image 18/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000018.jpg: 384x640 5 persons, 106.9ms\n",
      "image 19/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000019.jpg: 384x640 4 persons, 104.4ms\n",
      "image 20/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000020.jpg: 384x640 4 persons, 106.7ms\n",
      "image 21/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000021.jpg: 384x640 4 persons, 106.8ms\n",
      "image 22/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000022.jpg: 384x640 4 persons, 104.8ms\n",
      "image 23/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000023.jpg: 384x640 4 persons, 97.4ms\n",
      "image 24/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000024.jpg: 384x640 4 persons, 98.2ms\n",
      "image 25/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000025.jpg: 384x640 3 persons, 109.9ms\n",
      "image 26/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000026.jpg: 384x640 3 persons, 97.8ms\n",
      "image 27/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000027.jpg: 384x640 3 persons, 105.3ms\n",
      "image 28/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000028.jpg: 384x640 3 persons, 109.2ms\n",
      "image 29/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000029.jpg: 384x640 3 persons, 112.9ms\n",
      "image 30/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000030.jpg: 384x640 3 persons, 102.0ms\n",
      "image 31/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000031.jpg: 384x640 3 persons, 103.8ms\n",
      "image 32/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000032.jpg: 384x640 3 persons, 106.9ms\n",
      "image 33/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000033.jpg: 384x640 4 persons, 100.6ms\n",
      "image 34/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000034.jpg: 384x640 4 persons, 113.4ms\n",
      "image 35/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000035.jpg: 384x640 4 persons, 105.2ms\n",
      "image 36/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000036.jpg: 384x640 4 persons, 102.0ms\n",
      "image 37/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000037.jpg: 384x640 4 persons, 105.5ms\n",
      "image 38/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000038.jpg: 384x640 3 persons, 108.1ms\n",
      "image 39/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000039.jpg: 384x640 3 persons, 96.6ms\n",
      "image 40/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000040.jpg: 384x640 3 persons, 103.2ms\n",
      "image 41/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000041.jpg: 384x640 3 persons, 105.1ms\n",
      "image 42/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000042.jpg: 384x640 3 persons, 94.0ms\n",
      "image 43/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000043.jpg: 384x640 3 persons, 102.9ms\n",
      "image 44/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000044.jpg: 384x640 3 persons, 107.8ms\n",
      "image 45/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000045.jpg: 384x640 3 persons, 109.6ms\n",
      "image 46/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000046.jpg: 384x640 3 persons, 104.3ms\n",
      "image 47/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000047.jpg: 384x640 3 persons, 102.7ms\n",
      "image 48/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000048.jpg: 384x640 3 persons, 108.0ms\n",
      "image 49/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000049.jpg: 384x640 3 persons, 111.7ms\n",
      "image 50/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000050.jpg: 384x640 3 persons, 113.2ms\n",
      "image 51/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000051.jpg: 384x640 3 persons, 118.9ms\n",
      "image 52/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000052.jpg: 384x640 3 persons, 104.8ms\n",
      "image 53/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000053.jpg: 384x640 3 persons, 104.8ms\n",
      "image 54/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000054.jpg: 384x640 4 persons, 104.2ms\n",
      "image 55/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000055.jpg: 384x640 4 persons, 103.6ms\n",
      "image 56/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000056.jpg: 384x640 4 persons, 90.8ms\n",
      "image 57/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000057.jpg: 384x640 4 persons, 111.1ms\n",
      "image 58/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000058.jpg: 384x640 4 persons, 94.0ms\n",
      "image 59/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000059.jpg: 384x640 4 persons, 140.9ms\n",
      "image 60/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000060.jpg: 384x640 4 persons, 99.5ms\n",
      "image 61/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000061.jpg: 384x640 2 persons, 109.5ms\n",
      "image 62/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000062.jpg: 384x640 4 persons, 100.9ms\n",
      "image 63/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000063.jpg: 384x640 4 persons, 101.6ms\n",
      "image 64/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000064.jpg: 384x640 3 persons, 109.8ms\n",
      "image 65/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000065.jpg: 384x640 3 persons, 110.0ms\n",
      "image 66/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000066.jpg: 384x640 3 persons, 114.2ms\n",
      "image 67/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000067.jpg: 384x640 3 persons, 104.4ms\n",
      "image 68/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000068.jpg: 384x640 3 persons, 111.7ms\n",
      "image 69/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000069.jpg: 384x640 3 persons, 104.0ms\n",
      "image 70/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000070.jpg: 384x640 3 persons, 103.3ms\n",
      "image 71/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000071.jpg: 384x640 2 persons, 101.3ms\n",
      "image 72/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000072.jpg: 384x640 2 persons, 93.8ms\n",
      "image 73/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000073.jpg: 384x640 2 persons, 110.6ms\n",
      "image 74/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000074.jpg: 384x640 3 persons, 113.4ms\n",
      "image 75/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000075.jpg: 384x640 3 persons, 94.3ms\n",
      "image 76/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000076.jpg: 384x640 2 persons, 102.3ms\n",
      "image 77/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000077.jpg: 384x640 3 persons, 103.3ms\n",
      "image 78/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000078.jpg: 384x640 3 persons, 101.1ms\n",
      "image 79/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000079.jpg: 384x640 4 persons, 105.1ms\n",
      "image 80/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000080.jpg: 384x640 4 persons, 108.9ms\n",
      "image 81/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000081.jpg: 384x640 4 persons, 101.6ms\n",
      "image 82/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000082.jpg: 384x640 4 persons, 102.8ms\n",
      "image 83/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000083.jpg: 384x640 4 persons, 104.8ms\n",
      "image 84/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000084.jpg: 384x640 4 persons, 99.4ms\n",
      "image 85/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000085.jpg: 384x640 4 persons, 104.3ms\n",
      "image 86/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000086.jpg: 384x640 4 persons, 122.8ms\n",
      "image 87/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000087.jpg: 384x640 3 persons, 110.8ms\n",
      "image 88/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000088.jpg: 384x640 3 persons, 107.5ms\n",
      "image 89/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000089.jpg: 384x640 3 persons, 101.1ms\n",
      "image 90/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000090.jpg: 384x640 2 persons, 105.3ms\n",
      "image 91/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000091.jpg: 384x640 3 persons, 105.7ms\n",
      "image 92/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000092.jpg: 384x640 4 persons, 124.0ms\n",
      "image 93/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000093.jpg: 384x640 4 persons, 101.5ms\n",
      "image 94/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000094.jpg: 384x640 4 persons, 98.6ms\n",
      "image 95/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000095.jpg: 384x640 4 persons, 106.4ms\n",
      "image 96/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000096.jpg: 384x640 4 persons, 105.5ms\n",
      "image 97/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000097.jpg: 384x640 5 persons, 103.2ms\n",
      "image 98/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000098.jpg: 384x640 2 persons, 104.7ms\n",
      "image 99/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000099.jpg: 384x640 3 persons, 106.7ms\n",
      "image 100/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000100.jpg: 384x640 4 persons, 110.4ms\n",
      "image 101/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000101.jpg: 384x640 3 persons, 101.2ms\n",
      "image 102/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000102.jpg: 384x640 5 persons, 100.0ms\n",
      "image 103/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000103.jpg: 384x640 5 persons, 101.3ms\n",
      "image 104/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000104.jpg: 384x640 5 persons, 114.2ms\n",
      "image 105/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000105.jpg: 384x640 3 persons, 102.0ms\n",
      "image 106/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000106.jpg: 384x640 4 persons, 103.8ms\n",
      "image 107/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000107.jpg: 384x640 5 persons, 111.3ms\n",
      "image 108/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000108.jpg: 384x640 4 persons, 106.2ms\n",
      "image 109/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000109.jpg: 384x640 5 persons, 113.6ms\n",
      "image 110/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000110.jpg: 384x640 5 persons, 107.1ms\n",
      "image 111/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000111.jpg: 384x640 5 persons, 107.2ms\n",
      "image 112/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000112.jpg: 384x640 5 persons, 102.7ms\n",
      "image 113/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000113.jpg: 384x640 5 persons, 105.1ms\n",
      "image 114/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000114.jpg: 384x640 2 persons, 139.2ms\n",
      "image 115/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000115.jpg: 384x640 3 persons, 115.2ms\n",
      "image 116/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000116.jpg: 384x640 3 persons, 99.3ms\n",
      "image 117/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000117.jpg: 384x640 4 persons, 101.6ms\n",
      "image 118/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000118.jpg: 384x640 3 persons, 110.8ms\n",
      "image 119/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000119.jpg: 384x640 3 persons, 108.0ms\n",
      "image 120/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000120.jpg: 384x640 4 persons, 107.0ms\n",
      "image 121/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000121.jpg: 384x640 4 persons, 144.8ms\n",
      "[[0.1180274486541748, 0.07988494634628296, 0.09848171472549438, 0.10746544599533081, 0.13486987352371216, 0.12874603271484375, 0.07878190279006958, 0.09559839963912964, 0.07605499029159546, 0.08221262693405151, 0.12429952621459961, 0.11128789186477661, 0.09047681093215942, 0.1028713583946228, 0.08122843503952026]] [18, 12, 9, 16, 15, 1, 13, 11, 10, 6, 2, 7, 5, 4, 0]\n",
      "image 122/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000122.jpg: 384x640 5 persons, 103.0ms\n",
      "image 123/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000123.jpg: 384x640 5 persons, 404.4ms\n",
      "image 124/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000124.jpg: 384x640 6 persons, 409.3ms\n",
      "image 125/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000125.jpg: 384x640 5 persons, 392.5ms\n",
      "image 126/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000126.jpg: 384x640 5 persons, 127.9ms\n",
      "image 127/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000127.jpg: 384x640 5 persons, 112.1ms\n",
      "image 128/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000128.jpg: 384x640 5 persons, 112.2ms\n",
      "image 129/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000129.jpg: 384x640 4 persons, 122.1ms\n",
      "image 130/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000130.jpg: 384x640 3 persons, 126.6ms\n",
      "image 131/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000131.jpg: 384x640 3 persons, 137.5ms\n",
      "image 132/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000132.jpg: 384x640 3 persons, 106.6ms\n",
      "image 133/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000133.jpg: 384x640 4 persons, 130.7ms\n",
      "image 134/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000134.jpg: 384x640 3 persons, 111.9ms\n",
      "image 135/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000135.jpg: 384x640 3 persons, 111.2ms\n",
      "image 136/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000136.jpg: 384x640 3 persons, 127.6ms\n",
      "image 137/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000137.jpg: 384x640 3 persons, 187.7ms\n",
      "image 138/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000138.jpg: 384x640 3 persons, 125.2ms\n",
      "image 139/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000139.jpg: 384x640 3 persons, 112.3ms\n",
      "image 140/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000140.jpg: 384x640 3 persons, 116.5ms\n",
      "image 141/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000141.jpg: 384x640 3 persons, 122.9ms\n",
      "image 142/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000142.jpg: 384x640 3 persons, 108.9ms\n",
      "image 143/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000143.jpg: 384x640 3 persons, 117.5ms\n",
      "image 144/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000144.jpg: 384x640 4 persons, 129.5ms\n",
      "image 145/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000145.jpg: 384x640 4 persons, 108.7ms\n",
      "image 146/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000146.jpg: 384x640 4 persons, 107.9ms\n",
      "image 147/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000147.jpg: 384x640 4 persons, 129.9ms\n",
      "image 148/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000148.jpg: 384x640 4 persons, 112.3ms\n",
      "image 149/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000149.jpg: 384x640 4 persons, 113.5ms\n",
      "image 150/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000150.jpg: 384x640 4 persons, 120.3ms\n",
      "image 151/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000151.jpg: 384x640 5 persons, 112.7ms\n",
      "image 152/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000152.jpg: 384x640 5 persons, 111.8ms\n",
      "image 153/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000153.jpg: 384x640 4 persons, 137.5ms\n",
      "image 154/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000154.jpg: 384x640 4 persons, 140.3ms\n",
      "image 155/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000155.jpg: 384x640 4 persons, 131.4ms\n",
      "image 156/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000156.jpg: 384x640 4 persons, 128.3ms\n",
      "image 157/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000157.jpg: 384x640 5 persons, 117.3ms\n",
      "image 158/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000158.jpg: 384x640 6 persons, 95.4ms\n",
      "image 159/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000159.jpg: 384x640 6 persons, 102.8ms\n",
      "image 160/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000160.jpg: 384x640 6 persons, 100.5ms\n",
      "image 161/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000161.jpg: 384x640 4 persons, 101.6ms\n",
      "image 162/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000162.jpg: 384x640 5 persons, 114.5ms\n",
      "image 163/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000163.jpg: 384x640 5 persons, 122.1ms\n",
      "image 164/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000164.jpg: 384x640 5 persons, 106.3ms\n",
      "image 165/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000165.jpg: 384x640 5 persons, 107.2ms\n",
      "image 166/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000166.jpg: 384x640 6 persons, 103.7ms\n",
      "image 167/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000167.jpg: 384x640 5 persons, 106.0ms\n",
      "image 168/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000168.jpg: 384x640 5 persons, 116.1ms\n",
      "image 169/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000169.jpg: 384x640 4 persons, 113.0ms\n",
      "image 170/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000170.jpg: 384x640 4 persons, 104.4ms\n",
      "image 171/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000171.jpg: 384x640 4 persons, 100.6ms\n",
      "image 172/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000172.jpg: 384x640 4 persons, 118.0ms\n",
      "image 173/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000173.jpg: 384x640 5 persons, 646.7ms\n",
      "image 174/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000174.jpg: 384x640 6 persons, 561.7ms\n",
      "image 175/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000175.jpg: 384x640 5 persons, 369.0ms\n",
      "image 176/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000176.jpg: 384x640 4 persons, 312.5ms\n",
      "image 177/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000177.jpg: 384x640 4 persons, 327.5ms\n",
      "image 178/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000178.jpg: 384x640 5 persons, 331.7ms\n",
      "image 179/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000179.jpg: 384x640 5 persons, 316.0ms\n",
      "image 180/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000180.jpg: 384x640 4 persons, 100.3ms\n",
      "image 181/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000181.jpg: 384x640 4 persons, 95.6ms\n",
      "image 182/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000182.jpg: 384x640 3 persons, 109.1ms\n",
      "image 183/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000183.jpg: 384x640 4 persons, 98.7ms\n",
      "image 184/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000184.jpg: 384x640 4 persons, 100.2ms\n",
      "image 185/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000185.jpg: 384x640 2 persons, 100.9ms\n",
      "image 186/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000186.jpg: 384x640 4 persons, 98.7ms\n",
      "image 187/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000187.jpg: 384x640 4 persons, 96.0ms\n",
      "image 188/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000188.jpg: 384x640 3 persons, 106.2ms\n",
      "image 189/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000189.jpg: 384x640 3 persons, 98.8ms\n",
      "image 190/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000190.jpg: 384x640 2 persons, 110.3ms\n",
      "image 191/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000191.jpg: 384x640 2 persons, 94.9ms\n",
      "image 192/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000192.jpg: 384x640 2 persons, 97.8ms\n",
      "image 193/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000193.jpg: 384x640 2 persons, 97.1ms\n",
      "image 194/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000194.jpg: 384x640 2 persons, 93.9ms\n",
      "image 195/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000195.jpg: 384x640 2 persons, 99.2ms\n",
      "image 196/196 e:\\ML\\Pedestrian_trackingMOT\\MOT15\\train\\test4\\img1\\000196.jpg: 384x640 4 persons, 105.6ms\n",
      "Speed: 3.3ms preprocess, 124.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import configparser\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "\n",
    "dataset_name=\"test22\"\n",
    "\n",
    "def get_src(dataset_name):\n",
    "    src=\"MOT15/train/\"\n",
    "    data_source=src+dataset_name+\"/img1\"\n",
    "    config_src=src+dataset_name+\"/seqinfo.ini\"\n",
    "    return data_source,config_src\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "data_source,config_src=get_src(dataset_name)\n",
    "config.read(config_src)\n",
    "\n",
    "\n",
    "\n",
    "model = YOLO(r'ultralytics\\yolov8n.pt')\n",
    "results = model(data_source,stream=True,classes=[0],conf=0.65)\n",
    "tracker=Tracker(data_source)\n",
    "\n",
    "\n",
    "way=\"iou_net\"\n",
    "\n",
    "if way==\"iou\":\n",
    "    interval=math.ceil(float(config['Sequence']['frameRate'])*2)\n",
    "elif way==\"net\":\n",
    "    interval=math.ceil(float(config['Sequence']['frameRate'])/2)\n",
    "elif way==\"iou_net\":    \n",
    "    interval=math.ceil(float(config['Sequence']['frameRate'])*3)\n",
    "\n",
    "for r in results:\n",
    "\n",
    "    tracker.put_bbox(r.boxes.xyxy)\n",
    "    if way==\"iou\":\n",
    "        tracker.trackByIoU(iou=0.5,interval=interval)\n",
    "    elif way==\"net\":\n",
    "        tracker.trackBYnet(fine_tune=False,threshold=0.9,interval=interval)\n",
    "    elif way==\"iou_net\":\n",
    "        tracker.trackByIoU_net(iou=0.75,interval=interval,threshold=0.9224,fine_tune=True,net_type='vgg')\n",
    "\n",
    "\n",
    "tracker.save_result(r'E:\\ML\\Pedestrian_trackingMOT\\results\\IoU_vgg\\test4(fine_tune,vgg)')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# 文件夹路径和视频属性\n",
    "folder_path = 'MOT15/train/KITTI-17/img1'\n",
    "video_name = 'output_video_original.avi'\n",
    "frame_rate = 10 # 帧率\n",
    "frame_size = (1224, 370)  # 分辨率\n",
    "\n",
    "# 初始化OpenCV视频编写器\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video_writer = cv2.VideoWriter(video_name, fourcc, frame_rate, frame_size)\n",
    "\n",
    "# 遍历文件夹中的图片\n",
    "for file_name in sorted(os.listdir(folder_path)):\n",
    "    if file_name.endswith('.jpg') or file_name.endswith('.png'):\n",
    "        image_path = os.path.join(folder_path, file_name)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # 调整图片大小以匹配视频的分辨率\n",
    "        image = cv2.resize(image, frame_size)\n",
    "        \n",
    "        # 将图片写入视频  \n",
    "        video_writer.write(image)\n",
    "\n",
    "# 释放资源\n",
    "video_writer.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video_test33_0.85vgg.mp4.\n",
      "Moviepy - Writing video output_video_test33_0.85vgg.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video_test33_0.85vgg.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "import os\n",
    "\n",
    "# 设置图片所在的文件夹和输出视频的参数\n",
    "folder_path = r'results\\IoU_vgg\\test33(full-fine_tune,vgg)'\n",
    "output_video_path = 'output_video_test33_0.85vgg.mp4'\n",
    "fps =float(config['Sequence']['frameRate'])  # 帧率\n",
    "\n",
    "# 获取文件夹内所有图片的路径\n",
    "image_files = [os.path.join(folder_path, img) for img in os.listdir(folder_path) if img.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "image_files.sort()  # 确保文件是按顺序排列的\n",
    "\n",
    "# 创建视频剪辑并保存\n",
    "clip = ImageSequenceClip(image_files, fps=fps)\n",
    "clip.write_videofile(output_video_path, fps=fps)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
